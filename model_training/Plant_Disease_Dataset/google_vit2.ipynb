{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9f3d1-e4f9-4229-a35f-0a3db8f18812",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets timm safetensors\n",
    "\n",
    "print(\"danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6dbf0b-343d-4498-9fe6-bd934e375886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from safetensors.torch import save_file\n",
    "from IPython.display import FileLink\n",
    "\n",
    "print(\"danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e40244-e00f-467b-9e37-6eb1a1bf7b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dir = \"train\"\n",
    "valid_dir = \"valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c39ef3-58d2-4ac7-a7c0-e814aa8714b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danish\n"
     ]
    }
   ],
   "source": [
    "# 5. Load ViT Feature Extractor\n",
    "from transformers import ViTImageProcessor\n",
    "\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "print(\"Danish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26bd0b5c-bcc7-4393-91b9-b36405f2acd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected classes (40): ['.DS_Store', 'Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Random___image', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n"
     ]
    }
   ],
   "source": [
    "classes = sorted(os.listdir(train_dir))\n",
    "print(f\"Detected classes ({len(classes)}): {classes}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6567c92-c3b1-4768-a1ea-59fb90a23ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danish\n"
     ]
    }
   ],
   "source": [
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, class_names=None):\n",
    "        \"\"\"\n",
    "        :param root_dir: Path to the folder (train, valid, or test)\n",
    "        :param transform: Transformations to be applied\n",
    "        :param class_names: List of class names (subfolders)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_names = class_names\n",
    "        \n",
    "        # Gather all images and labels\n",
    "        for label, class_name in enumerate(self.class_names):\n",
    "            class_path = os.path.join(self.root_dir, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                # If the folder doesn't exist, skip\n",
    "                continue\n",
    "            \n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "print(\"Danish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09a7e95-1dc3-4695-8630-9887618c0c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danish\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "])\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = PlantDataset(root_dir=train_dir, transform=transform, class_names=classes)\n",
    "valid_dataset = PlantDataset(root_dir=valid_dir, transform=transform, class_names=classes)\n",
    "\n",
    "print(\"Danish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed79caa2-023f-4934-b1a4-cba0d890bcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danish\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print(\"Danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e2941ec-abd7-4a84-9d0e-cd8a2506b878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 76764\n",
      "Valid samples: 21062\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Valid samples: {len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4ed287d-11cb-4c76-a0c8-83a3482d119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danish\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=len(classes),\n",
    "    id2label={str(i): classes[i] for i in range(len(classes))},\n",
    "    label2id={classes[i]: i for i in range(len(classes))}\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b45e017-7440-4508-8549-81e2acfd936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "danish\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "print(\"danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428efbbb-600b-4e72-a4ca-e48a36b85541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, epochs=4):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Training loop\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\", leave=True)\n",
    "        for images, labels in train_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            train_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss_sum = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images).logits\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                val_loss_sum += val_loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_loss_avg = val_loss_sum / len(valid_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Valid Loss: {val_loss_avg:.4f} | Valid Acc: {val_acc:.2f}%\\n\")\n",
    "        \n",
    "        # Save checkpoint each epoch (if desired)\n",
    "        torch.save(model.state_dict(), f\"vit_epoch{epoch+1}.pt\")\n",
    "\n",
    "print(\"Danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691cefe-0e65-41c9-83da-b243720a3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "dataset_dir = \"train\"\n",
    "for class_name in os.listdir(dataset_dir):\n",
    "    class_path = os.path.join(dataset_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        try:\n",
    "            Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(\"Corrupted file:\", img_path, e)\n",
    "\n",
    "            \n",
    "print(\"danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae4b5d9-4f95-4728-813b-81dc18962c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 [Training]:   0%|                                                                     | 0/2399 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, valid_loader, epochs=4)\n",
    "\n",
    "\n",
    "print(\"Danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4615feb4-cc80-41e9-86bc-096bda6a37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f5ea6-4b0f-46b8-8b38-605384884eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url = \"https://projectbluearchive.blob.core.windows.net/media/Default/Potatoes/Disease%20and%20defects/Alternaria%20Leaf%20resized.jpg\"\n",
    "response = requests.get(sample_url)\n",
    "sample_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# Preprocess the image (same as training)\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "])\n",
    "input_image = inference_transform(sample_image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc221c3-768d-49a0-8454-37ea63542705",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(input_image).logits\n",
    "pred_idx = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "print(f\"Predicted class index: {pred_idx}\")\n",
    "print(f\"Predicted label: {classes[pred_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a48ca-8c25-4aad-9dc0-bc95dc4a5b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"plant_disease_model.safetensors\"\n",
    "save_file(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Provide a download link (for environments like JupyterLab or Kaggle)\n",
    "FileLink(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f26f9-6eaf-4c5b-ba36-a38aa02f928d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
